# 摘自极客时间JAVA并发编程实战，想深入学习可自行阅读

# 01. 可见性、原子性和有序性问题：并发编程Bug的源头
CPU、内存、I/O设备存在速度差异，`CPU是天上一天，内存是地上一年。内存和I/O设备的速度差异就更大了，内存是天上一天，I/O设备是地上十年`。根据木桶理论（一只水桶能装多少水取决于它最短的那块木板），程序整体的性能取决于最慢的操作——读写I/O设备。  

为了合理利用CPU的高性能，平衡这三者的速度差异，计算机体系机构、操作系统、编译程序都做出了贡献，主要体现为：
+ CPU增加了缓存，以均衡与内存的速度差异；
+ 操作系统增加了进程、线程，以分时复用CPU，进而均衡CPU与I/O设备的速度差异；
+ 编译程序优化指令执行次序，使得缓存能够得到更加合理地利用。

同时上述优化也带来了并发问题
## 缓存导致的可见性问题
一个线程对共享变量的修改，另外一个线程能够立刻看到，我们称为可见性。多个线程在不同cpu的缓存中变量不可见是缓存造成的主要问题
## 线程切换带来的原子性问题
操作系统允许某个进程执行一小段时间，例如50毫秒，过了50毫秒操作系统就会重新选择一个进程来执行（我们称为“任务切换”，现在的操作系统基本都是基于线程的调度，所以通常就指“线程切换”），这个50毫秒称为`时间片`。我们把一个或者多个操作在CPU执行的过程中不被中断的特性称为`原子性`。线程切换往往带来原子性问题。
## 编译优化带来的有序性问题
编译器为了优化性能，有时候会改变程序中语句的先后顺序，可能会产生有序性问题。

## 总结

要写好并发程序，首先要知道并发程序的问题在哪里，只有确定了“靶子”，才有可能把问题解决，毕竟所有的解决方案都是针对问题的。并发程序经常出现的诡异问题看上去非常无厘头，但是深究的话，无外乎就是直觉欺骗了我们，**只要我们能够深刻理解可见性、原子性、有序性在并发场景下的原理，很多并发Bug都是可以理解、可以诊断的。**

缓存导致的可见性问题，线程切换带来的原子性问题，编译优化带来的有序性问题，其实缓存、线程、编译优化的目的和我们写并发程序的目的是相同的，都是提高程序性能。但是技术在解决一个问题的同时，必然会带来另外一个问题，所以在采用一项技术的同时，一定要清楚它带来的问题是什么，以及如何规避。

---
# 02. Java内存模型：看Java如何解决可见性和有序性问题
`导致可见性的原因是缓存`，`导致有序性的原因是编译优化`，那解决可见性、有序性最直接的办法就是`按需禁用缓存以及编译优化`。`Java内存模型`规范了JVM如何提供按需禁用缓存和编译优化的方法。具体来说，这些方法包括 volatile、synchronized 和 final 三个关键字，以及六项 Happens-Before 规则。
## Happens-Before 规则
1. `程序的顺序性规则`（按照程序顺序，前面的操作 Happens-Before 于后续的任意操作）
2. `volatile变量规则`（对一个volatile变量的写操作， Happens-Before 于后续对这个volatile变量的读操作）
3. `传递性`（如果A Happens-Before B，且B Happens-Before C，那么A Happens-Before C）
4. `管程中锁的规则`（锁的解锁 Happens-Before 于后续对这个锁的加锁）
5. `线程 start() 规则`（主线程A启动子线程B后，子线程B能够看到主线程在启动子线程B前的操作）
6. `线程 join() 规则`（主线程A等待子线程B完成（主线程A通过调用子线程B的join()方法实现），当子线程B完成后（主线程A中join()方法返回），主线程能够看到子线程的操作）

在Java语言里面，Happens-Before的语义本质上是一种可见性，A Happens-Before B 意味着A事件对B事件来说是可见的，无论A事件和B事件是否发生在同一个线程里。例如A事件发生在线程1上，B事件发生在线程2上，Happens-Before规则保证线程2上也能看到A事件的发生。

# 3&4&5. 互斥锁
`原子性问题的源头是线程切换`,单核CPU可以通过按需禁止线程切换来解决原子问题，但是多核环境下则禁止CPU中断，只能保证CPU上的线程连续执行，并不能保证同一时刻只有一个线程执行。如果能够保证对共享变量的修改是`互斥`（同一时刻只有一个线程执行）的，那么，无论是单核CPU还是多核CPU，就都能保证原子性了。  
锁是一种通用的技术方案，Java语言提供的synchronized关键字，就是锁的一种实现。synchronized关键字可以用来修饰方法，也可以用来修饰代码块，它的使用示例基本上都是下面这个样子：
```
class X {
  // 修饰非静态方法
  synchronized void foo() {
    // 临界区
  }
  // 修饰静态方法
  synchronized static void bar() {
    // 临界区
  }
  // 修饰代码块
  Object obj = new Object()；
  void baz() {
    synchronized(obj) {
      // 临界区
    }
  }
}  
```
在Java的一条隐式规则：
1. 当修饰静态方法的时候，锁定的是当前类的Class对象，在上面的例子中就是Class X；
2. 当修饰非静态方法的时候，锁定的是当前实例对象this。

对于上面的例子，synchronized修饰静态方法相当于:
```
class X {
  // 修饰静态方法
  synchronized(X.class) static void bar() {
    // 临界区
  }
}
```
修饰非静态方法，相当于：
```
class X {
  // 修饰非静态方法
  synchronized(this) void foo() {
    // 临界区
  }
}
```
## 保护没有关联关系的多个资源
受保护资源和锁之间合理的关联关系应该是N:1的关系，可以用一把锁来保护多个资源，但是不能用多把锁来保护一个资源。分析下面代码

```
 class Account {
  // 锁：保护账户余额
  private final Object balLock
    = new Object();
  // 账户余额  
  private Integer balance;
  // 锁：保护账户密码
  private final Object pwLock
    = new Object();
  // 账户密码
  private String password;

  // 取款
  void withdraw(Integer amt) {
    synchronized(balLock) {
      if (this.balance > amt){
        this.balance -= amt;
      }
    }
  } 
  // 查看余额
  Integer getBalance() {
    synchronized(balLock) {
      return balance;
    }
  }

  // 更改密码
  void updatePassword(String pw){
    synchronized(pwLock) {
      this.password = pw;
    }
  } 
  // 查看密码
  String getPassword() {
    synchronized(pwLock) {
      return password;
    }
  }
}   
```

可以用一把互斥锁来保护多个资源，例如我们可以用this这一把锁来管理账户类里所有的资源：账户余额和用户密码。具体实现很简单，示例程序中所有的方法都增加同步关键字synchronized就可以了，这里我就不一一展示了。

但是用一把锁有个问题，就是性能太差，会导致取款、查看余额、修改密码、查看密码这四个操作都是串行的。而我们用两把锁，取款和修改密码是可以并行的。用不同的锁对受保护资源进行精细化管理，能够提升性能。这种锁还有个名字，叫`细粒度锁`。
## 保护有关联关系的多个资源
根据前面所学，直觉上代码如下：
```
class Account {
  private int balance;
  // 转账
  synchronized void transfer(
      Account target, int amt){
    if (this.balance > amt) {
      this.balance -= amt;
      target.balance += amt;
    }
  } 
}
```
上述代码问题出在this这把锁可以保护自己的余额this.balance，却保护不了别人的余额target.balance。正确的做法是创建Account对象时，传入相同的lock，这样所有的Account对象都会共享这个lock了。代码如下：

```
class Account {
  private Object lock；
  private int balance;
  private Account();
  // 创建Account时传入同一个lock对象
  public Account(Object lock) {
    this.lock = lock;
  } 
  // 转账
  void transfer(Account target, int amt){
    // 此处检查所有对象共享的锁
    synchronized(lock) {
      if (this.balance > amt) {
        this.balance -= amt;
        target.balance += amt;
      }
    }
  }
}
```
这样又带来问题：所有并行的操作都变成了串行的，当有大量交易进行时，系统性能就会急剧下降。

## 死锁的产生
为了进一步优化，可用两把锁就实现了，转出账本一把，转入账本另一把。在transfer()方法内部，我们首先尝试锁定转出账户this（先把转出账本拿到手），然后尝试锁定转入账户target（再把转入账本拿到手），只有当两者都成功时，才执行转账操作。

优化代码如下：
```
class Account {
  private int balance;
  // 转账
  void transfer(Account target, int amt){
    // 锁定转出账户
    synchronized(this) {              
      // 锁定转入账户
      synchronized(target) {           
        if (this.balance > amt) {
          this.balance -= amt;
          target.balance += amt;
        }
      }
    }
  } 
}
```
这是`细粒度锁`的应用。使用细粒度锁可以提高并行度，是性能优化的一个重要手段。但是使用细粒度锁是有代价的，这个代价就是可能会导致死锁。  
`死锁`的一个比较专业的定义是：一组互相竞争资源的线程因互相等待，导致“永久”阻塞的现象。

如果用上述代码实现转账功能，当A转钱给B，同时B转钱给A，正好在A锁定自己的同时B也锁定自己，这是A等待B的锁被释放，B等待A的锁被释放，这样就会无限等待下去，导致两个交易都无法进行，就造成了死锁。

## 如何预防死锁
以下这四个条件同时发生时才会出现死锁：
1. 互斥，共享资源X和Y只能被一个线程占用；
2. 占有且等待，线程T1已经取得共享资源X，在等待共享资源Y的时候，不释放共享资源X；
3. 不可抢占，其他线程不能强行抢占线程T1占有的资源；
4. 循环等待，线程T1等待线程T2占有的资源，线程T2等待线程T1占有的资源，就是循环等待。

反过来分析，也就是说只要我们`破坏其中一个，就可以成功避免死锁的发生`。其中，互斥这个条件我们没有办法破坏，因为我们用锁为的就是互斥，下面我们破坏其他条件。
### 破坏占用且等待条件
从理论上讲，要破坏这个条件，可以一次性申请所有资源，对应到编程领域，“同时申请”这个操作是一个临界区，我们也需要一个角色（Java里面的类）来管理这个临界区，我们就把这个角色定为Allocator。它有两个重要功能，分别是：同时申请资源apply()和同时释放资源free()。账户Account 类里面持有一个Allocator的单例（必须是单例，只能由一个人来分配资源）。当账户Account在执行转账操作的时候，首先向Allocator同时申请转出账户和转入账户这两个资源，成功后再锁定这两个资源；当转账操作执行完，释放锁之后，我们需通知Allocator同时释放转出账户和转入账户这两个资源。具体的代码实现如下。

```
class Allocator {
  private List<Object> als =
    new ArrayList<>();
  // 一次性申请所有资源
  synchronized boolean apply(
    Object from, Object to){
    if(als.contains(from) || als.contains(to)){
      return false;  
    } else {
      als.add(from);
      als.add(to);  
    }
    return true;
  }
  // 归还资源
  synchronized void free(
    Object from, Object to){
    als.remove(from);
    als.remove(to);
  }
}

class Account {
  // actr应该为单例
  private Allocator actr;
  private int balance;
  // 转账
  void transfer(Account target, int amt){
    // 一次性申请转出账户和转入账户，直到成功
    while(!actr.apply(this, target))；
    try{
      // 锁定转出账户
      synchronized(this){              
        // 锁定转入账户
        synchronized(target){           
          if (this.balance > amt){
            this.balance -= amt;
            target.balance += amt;
          }
        }
      }
    } finally {
      actr.free(this, target)
    }
  } 
}
```
### 破坏不可抢占条件
破坏不可抢占条件看上去很简单，核心是要能够主动释放它占有的资源，这一点synchronized是做不到的。原因是synchronized申请资源的时候，如果申请不到，线程直接进入阻塞状态了，而线程进入阻塞状态，啥都干不了，也释放不了线程已经占有的资源。Java在语言层次确实没有解决这个问题，不过在SDK层面还是解决了的，java.util.concurrent这个包下面提供的Lock是可以轻松解决这个问题的
### 破坏循环等待条件
破坏这个条件，需要对资源进行排序，然后按序申请资源（`该方法是将资源排成链，如果资源不能形成闭环就不会形成死锁`）。这个实现非常简单，我们假设每个账户都有不同的属性 id，这个 id 可以作为排序字段，申请的时候，我们可以按照从小到大的顺序来申请。比如下面代码中，1~6处的代码对转出账户（this）和转入账户（target）排序，然后按照序号从小到大的顺序锁定账户。这样就不存在“循环”等待了。
```
class Account {
  private int id;
  private int balance;
  // 转账
  void transfer(Account target, int amt){
    Account left = this        1
    Account right = target;    2
    if (this.id > target.id) { 3
      left = target;           4
      right = this;            5
    }                          6
    // 锁定序号小的账户
    synchronized(left){
      // 锁定序号大的账户
      synchronized(right){ 
        if (this.balance > amt){
          this.balance -= amt;
          target.balance += amt;
        }
      }
    }
  } 
}
```

# 06. 用“等待-通知”机制优化循环等待
上一篇文章你应该已经知道，在破坏占用且等待条件的时候，如果转出账本和转入账本不满足同时在文件架上这个条件，就用死循环的方式来循环等待，核心代码如下：
```
// 一次性申请转出账户和转入账户，直到成功
while(!actr.apply(this, target))；
```
当apply()操作耗时长，或者并发冲突量大的时候，循环等待这种方案就不适用了，因为在这种场景下，可能要循环上万次才能获取到锁，太消耗CPU了。  
`解决方案`：如果线程要求的条件（转出账本和转入账本同在文件架上）不满足，则线程阻塞自己，进入等待状态；当线程要求的条件（转出账本和转入账本同在文件架上）满足后，通知等待的线程重新执行。其中，使用线程阻塞的方式就能避免循环等待消耗CPU的问题,这就是`等待通知机制`。

`一个完整的等待-通知机制`：线程首先获取互斥锁，当线程要求的条件不满足时，释放互斥锁，进入等待状态；当要求的条件满足时，通知等待的线程，重新获取互斥锁。

## 等待-通知机制实现

在这个等待-通知机制中，我们需要考虑以下四个要素。
1. 互斥锁：上一篇文章我们提到Allocator需要是单例的，所以我们可以用this作为互斥锁。
2. 线程要求的条件：转出账户和转入账户都没有被分配过。
3. 何时等待：线程要求的条件不满足就等待。
4. 何时通知：当有线程释放账户时就通知。

将上面几个问题考虑清楚，可以快速完成下面的代码。需要注意的是我们使用了：
```
  while(条件不满足) {
    wait();
  }
```
利用这种范式可以解决上面提到的条件曾经满足过这个问题。因为当wait()返回时，有可能条件已经发生变化了，曾经条件满足，但是现在已经不满足了，所以要重新检验条件是否满足。范式，意味着是经典做法，所以没有特殊理由不要尝试换个写法。
```
class Allocator {
  private List<Object> als;
  // 一次性申请所有资源
  synchronized void apply(
    Object from, Object to){
    // 经典写法
    while(als.contains(from) ||
         als.contains(to)){
      try{
        wait();
      }catch(Exception e){
      }   
    } 
    als.add(from);
    als.add(to);  
  }
  // 归还资源
  synchronized void free(
    Object from, Object to){
    als.remove(from);
    als.remove(to);
    notifyAll();
  }
}
```

notify()是会随机地通知等待队列中的一个线程，而notifyAll()会通知等待队列中的所有线程。在实际应用中尽量使用notifyAll()，notify()可能导致某些线程永远不会被通知到。


# 07. 安全性、活跃性以及性能问题
并发编程中我们需要注意的问题主要有三个方面，分别是：`安全性问题、活跃性问题和性能问题`。

## 安全性
存在共享数据并且该数据会发生变化，通俗地讲就是有多个线程会同时读写同一数据。当多个线程同时访问同一数据，并且至少有一个线程会写这个数据的时候，如果我们不采取防护措施，就会产生`数据竞争`。

`竞态条件`，指的是程序的执行结果依赖线程执行的顺序

## 活跃性问题
所谓活跃性问题，指的是某个操作无法执行下去。我们常见的“死锁”就是一种典型的活跃性问题，除了死锁外，还有两种情况，分别是“活锁”和“饥饿”。

有时线程虽然没有发生阻塞，但仍然会存在执行不下去的情况，这就是所谓的“`活锁`”。解决“活锁”的方案很简单，谦让时，尝试等待一个随机的时间就可以了。等待一个随机时间”的方案虽然很简单，却非常有效，Raft这样知名的分布式一致性算法中也用到了它。

那“饥饿”该怎么去理解呢？所谓“饥饿”指的是线程因无法访问所需资源而无法执行下去的情况。解决“饥饿”问题的方案很简单，有三种方案：一是保证资源充足，二是公平地分配资源，三就是避免持有锁的线程长时间执行。这三个方案中，方案一和方案三的适用场景比较有限，因为很多场景下，资源的稀缺性是没办法解决的，持有锁的线程执行的时间也很难缩短。倒是方案二的适用场景相对来说更多一些。在并发编程里，主要是使用公平锁。所谓公平锁，是一种先来后到的方案，线程的等待是有顺序的，排在等待队列前面的线程会优先获得资源。

## 性能问题
使用“锁”要非常小心，但是如果小心过度，也可能出“性能问题”。“锁”的过度使用可能导致串行化的范围过大，这样就不能够发挥多线程的优势了，而我们之所以使用多线程搞并发程序，为的就是提升性能。

所以使用锁的时候一定要关注对性能的影响。 那怎么才能避免锁带来的性能问题呢？这个问题很复杂，Java SDK并发包里之所以有那么多东西，有很大一部分原因就是要提升在某个特定领域的性能。

不过从方案层面，我们可以这样来解决这个问题。

> 第一，既然使用锁会带来性能问题，那最好的方案自然就是使用无锁的算法和数据结构了。在这方面有很多相关的技术，例如线程本地存储(Thread Local Storage, TLS)、写入时复制(Copy-on-write)、乐观锁等；Java并发包里面的原子类也是一种无锁的数据结构；Disruptor则是一个无锁的内存队列，性能都非常好……  
第二，减少锁持有的时间。互斥锁本质上是将并行的程序串行化，所以要增加并行度，一定要减少持有锁的时间。这个方案具体的实现技术也有很多，例如使用细粒度的锁，一个典型的例子就是Java并发包里的ConcurrentHashMap，它使用了所谓分段锁的技术（这个技术后面我们会详细介绍）；还可以使用读写锁，也就是读是无锁的，只有写的时候才会互斥。

性能方面的度量指标有很多，我觉得有三个指标非常重要，就是：吞吐量、延迟和并发量。

> 吞吐量：指的是单位时间内能处理的请求数量。吞吐量越高，说明性能越好。  
延迟：指的是从发出请求到收到响应的时间。延迟越小，说明性能越好。  
并发量：指的是能同时处理的请求数量，一般来说随着并发量的增加、延迟也会增加。所以延迟这个指标，一般都会是基于并发量来说的。例如并发量是1000的时候，延迟是50毫秒。